{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Developer: Sanju Sarkar\n",
        "\n",
        "Email ID: sanjusarkar44@hotmail.com\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Eu6lpBrki7Qu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides a step-by-step guide for converting machine learning models into the GGUF (GPTQ-GGUF Unified Format) and loading them into Ollama for efficient local execution. The process includes downloading a model, converting it to GGUF format, and setting up Ollama for inference. This guide is useful for running optimized models on consumer hardware with minimal setup, leveraging Ollamaâ€™s streamlined model management and execution capabilities.\n"
      ],
      "metadata": {
        "id": "ImJYVaW9i0xz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Login with Huggingface Access Token (Required when model need authorization)"
      ],
      "metadata": {
        "id": "VoTdO2DCjZcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "ue91wIU3nZMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Dependency and Downloading Model"
      ],
      "metadata": {
        "id": "rk4yzEoljiX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-hub"
      ],
      "metadata": {
        "id": "ZlBP933adTqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud0mC1Xt8Xdb"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "model_id=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"  # Replace with the ID of the model you want to download.\n",
        "snapshot_download(repo_id=model_id, local_dir=\"model\") # Replace the local dir of your desired model downloading path."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cloning Llama.cpp GitHub Repo"
      ],
      "metadata": {
        "id": "JUBG_mEGj0mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp"
      ],
      "metadata": {
        "id": "q8JFBlM29Ocu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting the model into GGUF format."
      ],
      "metadata": {
        "id": "VRsKpiIIj8F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !python {path to convert_hf_to_gguf.py} {path to hf_model} --outfile {name_of_outputfile.gguf} --outtype {quantization type}\n",
        "\n",
        "!python /content/llama.cpp/convert_hf_to_gguf.py /content/model --outfile DeepSeek_R1_Distill_Qwen_1.5B.gguf --outtype q8_0"
      ],
      "metadata": {
        "id": "7ADRMB2W9Vml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading Ollama (Omni-Layer Learning Language Acquisition Model)"
      ],
      "metadata": {
        "id": "WZe-ArrRkH0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "PAsTQXOkkGZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Starting Ollama Server in Colab Terminal"
      ],
      "metadata": {
        "id": "CYVHU8Zokp6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm"
      ],
      "metadata": {
        "id": "_0vyt_ndeR5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext colabxterm\n",
        "%xterm"
      ],
      "metadata": {
        "id": "ZGbtRX6CfTip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating a sample ```Modelfile``` from a model available in Ollama."
      ],
      "metadata": {
        "id": "LtBBVJDGk4RK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will pull a model from ollama."
      ],
      "metadata": {
        "id": "Bqh7E0Tsla5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull deepseek-r1:1.5b"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EgkJLAnwf_LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelfile is the blueprint that Ollama uses to create and run models.This command generates a Modelfile based on the model specifications which I already had locally pulled."
      ],
      "metadata": {
        "id": "J1zfvvl-lS5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama show --modelfile deepseek-r1:1.5b >>  Modelfile"
      ],
      "metadata": {
        "id": "pRprIrS2f2UY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to modify the Modelfile to point to our downloaded GGUF model. Open the Modelfile in a text editor and update the FROM line with the path to the downloaded model."
      ],
      "metadata": {
        "id": "p6YoXqY7lxgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### It will Create the Model in Ollama"
      ],
      "metadata": {
        "id": "K2MxtcEKlf4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!ollama create {model_name_to_be_set} -f Modelfile\n",
        "!ollama create deepseek -f Modelfile"
      ],
      "metadata": {
        "collapsed": true,
        "id": "emoWZ4uPgNvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify the list"
      ],
      "metadata": {
        "id": "B2JpG0OQmVe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "id": "tZdqUDcfhUBh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}